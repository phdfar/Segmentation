INPUT:
  IMAGE_MEAN: [102.9801, 115.9465, 122.7717]  # [0.485, 0.456, 0.406]
  IMAGE_STD: [1.0, 1.0, 1.0]  # [0.229, 0.224, 0.225]
  NORMALIZE_TO_UNIT_SCALE: False
  MIN_DIM: 800
  MAX_DIM: 1333
  BGR_INPUT: True
  NUM_FRAMES: 8
  NUM_CLASSES: 2  # including background
TRAINING:
  MODE: ""
  LOSS_AT_FULL_RES: False
  FREEZE_BACKBONE: False
  MIXED_PRECISION: False
  MIXED_PRECISION_OPT_LEVEL: "O1"
  BATCH_SIZE: 2
  WEIGHT_DECAY: 0.0001
  MAX_ITERATIONS: 120000
  ACCUMULATE_GRADIENTS: True
  MAX_SAMPLES_PER_GPU: 1
  CLIP_GRADIENTS: False
  OPTIMIZER: "SGD"
  INITIAL_LR: 0.001
  LR_DECAY_TYPE: "step"  # "step" or "exponential" or "none"
  LR_DECAY_STEPS: [20000, 50000]
  LR_DECAY_FACTOR: 0.1
  LR_EXP_DECAY_FACTOR: 0.001
  LR_EXP_DECAY_START: 40000
  LR_EXP_DECAY_STEPS: 60000
  MOMENTUM: 0.9
  NESTEROV: True
  LOSSES:
    SEMSEG: "CrossEntropy"
    WEIGHT_SEMSEG: 1.0
    EMBEDDING:
      WEIGHT_REGULARIZATION: 0.001
      WEIGHT_LOVASZ: 1.0
      WEIGHT_VARIANCE_SMOOTHNESS: 10.0
      WEIGHT_SEEDINESS: 1.0
      WEIGHT: 1.0
      FREE_DIM_STDS: []
MODEL:
  USE_SEMSEG_HEAD: True
  USE_SEEDINESS_HEAD: False
  EMBEDDING_DIM_MODE: "xyt"  # "ff", "xy", "xyt", "xyf", "xyff", "xyfff"
  BACKBONE:
    PRETRAINED_WEIGHTS: "mask_rcnn_R_101_FPN_backbone.pth"
    TYPE: "R-101-FPN"
    FREEZE_AT_STAGE: 2
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    NUM_GROUPS: 1
    RES2_OUT_CHANNELS: 256
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  FPN:
    USE_GN: False
    USE_RELU: False
  EMBEDDINGS:
    HEAD_TYPE: "squeeze_expand_decoder"
    INTER_CHANNELS: [256, 256, 128, 128]
    SCALE: [32, 16, 8, 4]
    EMBEDDING_SIZE: 3
    TANH_ACTIVATION: True
    TIME_SCALE: 0.1
    NORMALIZATION_LAYER: "gn"
    GN_NUM_GROUPS: 32
    POOL_TYPE: "avg"
  SEMSEG:
    HEAD_TYPE: "squeeze_expand_decoder"
    FEATURE_SCALE: [4, 8, 16, 32]
    INTER_CHANNELS: [256, 256, 128, 128]
    NORMALIZATION_LAYER: "gn"
    GN_NUM_GROUPS: 32
    POOL_TYPE: "avg"
    FOREGROUND_CHANNEL: True
  SEEDINESS:
    HEAD_TYPE: "squeeze_expand_decoder"
    FEATURE_SCALE: [32, 16, 8, 4]
    INTER_CHANNELS: [256, 256, 128, 128]
    NORMALIZATION_LAYER: "gn"
    GN_NUM_GROUPS: 32
    POOL_TYPE: "avg"
DATA:
  DAVIS:
    FRAME_GAP_LOWER: 16
    FRAME_GAP_UPPER: 16
    SINGLE_INSTANCE_DUPLICATION: False
    MAX_INFERENCE_TRACKS: 20
    INFERENCE_FRAME_OVERLAP: 6
    COCO_WEIGHT: 0.25
    PASCAL_VOC_WEIGHT: 0.1
    YOUTUBE_VIS_WEIGHT: 0.35
    DAVIS_WEIGHT: 0.3
  YOUTUBE_VIS:
    FRAME_GAP_LOWER: 8
    FRAME_GAP_UPPER: 8
    SINGLE_INSTANCE_DUPLICATION: True
    MAX_INFERENCE_TRACKS: 10
    INFERENCE_FRAME_OVERLAP: 4
    COCO_WEIGHT: 0.3
    PASCAL_VOC_WEIGHT: 0.1
    YOUTUBE_VIS_WEIGHT: 0.6
  KITTI_MOTS:
    FRAME_GAP_LOWER: 8
    FRAME_GAP_UPPER: 8
    TRAIN_SEQS: ["0000", "0001", "0003", "0004", "0005", "0009", "0011", "0012", "0015", "0017", "0019", "0020", "0050"] # Sequence '0050' here is actually sequence '0002' from the MOTSChallenge dataset.
    VAL_SEQS: ["0002", "0006", "0007", "0008", "0010", "0013", "0014", "0016", "0018"]
    INFERENCE_FRAME_OVERLAP: 4
    MAX_INFERENCE_TRACKS: 1000  # effectively unlimited
    MAPILLARY_WEIGHT: 0.0
    KITTI_MOTS_WEIGHT: 1.0
CLUSTERING:
  MIN_SEEDINESS_PROB: 0.8
  PRIMARY_PROB_THRESHOLD: 0.5
  SECONDARY_PROB_THRESHOLD: 0.3